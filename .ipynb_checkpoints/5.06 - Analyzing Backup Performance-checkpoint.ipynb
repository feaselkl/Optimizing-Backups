{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Backup Performance\n",
    "\n",
    "There are several knobs we can turn to tune backup performance, but there isn't a great deal of guidance on what the best settings are for our environment, other than \"try them and see!\"  A big part of this is that the underlying hardware makes so much of a difference:  being I/O bound on backups means you want to configure things differently from if you are CPU-bound.  Also, if you are backing up a very busy system, you don't want to make the backup so well-tuned that it suddenly takes up 100% of your CPU.  This leads to a series of tradeoffs in configurable settings.\n",
    "\n",
    "The most important of those settings are:\n",
    "* Block Size -- The physical block size.  This really only matters for backup tapes and CD-ROMs but it is still settable.  Valid values:  { 0.5kb, 1kb, 2kb, 4kb, 8kb, 16kb, 32kb, 64kb }\n",
    "* Max Transfer Size -- Maximum amount of data to be transferred per operation.  Valid values:  { 64kb, 128kb, 256kb, 512kb, 1mb, 2mb, 4mb }\n",
    "* Buffer Count -- Number of buffers of size [MaxTransferSize] to be created.  Valid values:  { 1:N } but I probably wouldn't go above about 1024 without good reason, as with a MaxTransferSize of 4MB, that's up to 4GB of memory used for a single backup.\n",
    "* File Count -- Tell SQL Server to stripe your backup across multiple files.  This is a nice way of getting extra throughput out of your backups.  Valid values:  { 1:N } but I probably wouldn't go above 8.\n",
    "* Compression -- Tell SQL Server whether or not you want to compress your backup.  This has a very minor cost of CPU but typically leads to **much** smaller backups, so my default is to say yes.  Valid values:  { TRUE, FALSE }\n",
    "\n",
    "Armed with this knowledge, let's say you now want to go tune your systems.  Well, there are a **lot** of combinations.  Let's suppose that we go with the following options:\n",
    "* Block Size:  { 0.5kb, 1kb, 2kb, 4kb, 8kb, 16kb, 32kb, 64kb }\n",
    "* Max Transfer Size:  { 64kb, 128kb, 256kb, 512kb, 1mb, 2mb, 4mb }\n",
    "* Buffer Count:  { 7, 15, 30, 60, 128, 256, 512, 1024 }\n",
    "* File Count:  { 1, 2, 4 }\n",
    "* Compression:  { TRUE }\n",
    "\n",
    "This gives us 1344 separate options.  If your full backup averages 10 minutes, that's an expectation of 224 hours straight of backups to try each of these options.  If you have a terabyte-sized backup which takes 90 minutes to complete, you'll get your answer in approximately 84 days.\n",
    "\n",
    "But there's a not-so-secret weapon we can use:  sampling.  Without getting into the statistics of the problem, we can decide to take a random sample of the full set of options and perform an analysis on it.  With a reasonable-sized sample, we can get somewhere close to the actual population values in a fraction of the time.\n",
    "\n",
    "My sample today is from two databases, one which is 17GB in size and the other which is 1.5GB in size.  I built a Powershell script which builds a Cartesian product of my input arrays (that is, the parameters I laid out above) and runs the [dbatools](https://dbatools.io) cmdlet Backup-DbaDatabase.  I'm writing the output results to an output file.  Then, I manually added a header with the variable names to make it easier to import into R.  I'm sampling the Cartesian product, performing only about 5-6% of the total number of tests.  That's still a lot of tests, but it's a much more tractable problem.\n",
    "\n",
    "If you're interested in the Powershell code, here it is:\n",
    "\n",
    "```\n",
    "# https://gist.github.com/letmaik/d650ee257a27df8eac0f71f17aa99765\n",
    "function CartesianProduct {\n",
    "    param\n",
    "    (\n",
    "        [Parameter(Mandatory = $true, ValueFromPipeline = $true)]\n",
    "        [Hashtable]\n",
    "        $values = @{ Foo = 1..5; Bar = 1..10}\n",
    "    )\n",
    "    $keys = @($values.GetEnumerator() | ForEach-Object { $_.Name })\n",
    "    $result = @($values[$keys[0]] | ForEach-Object { @{ $keys[0] = $_ } })\n",
    "    if ($keys.Length -gt 1) {\n",
    "        foreach ($key in $keys[1..($keys.Length - 1)]) {\n",
    "            $result = foreach ($entry in $result) {\n",
    "                foreach ($value in $values[$key]) {\n",
    "                    $entry + @{ $key = $value }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    $result\n",
    "}\n",
    "\n",
    "$InstanceName = \"localhost\"\n",
    "$DatabaseName = \"StackOverflowTiny\"\n",
    "\n",
    "foreach ($entry in CartesianProduct @{ BlockSize = (0.5kb, 1kb, 2kb, 4kb, 8kb, 16kb, 32kb, 64kb); BufferCount = (7, 15, 30, 60, 128, 256, 512, 1024); MaxTransferSize = (64kb, 128kb, 256kb, 512kb, 1mb, 2mb, 4mb); FileCount = (1, 2, 4) }) {\n",
    "  # Sample:  1344 (8 * 8 * 7 * 3) possible entries. Let's get ~70 samples per database, so 5% of the total samples.\n",
    "  $rand = Get-Random -Maximum 100\n",
    "  # We are making a fair assumption that Get-Random is a uniformly distributed pseudo-random number generator.  Setting -Maximum 100 means we'll get a range from 0-99.\n",
    "  if ($rand -gt 94) {\n",
    "    $outcome = Backup-DbaDatabase -SqlInstance ($InstanceName) -BackupDirectory C:\\temp\\BackupFiles -Database ($DatabaseName) -Type Full -CopyOnly -CompressBackup -BufferCount ($entry.BufferCount) -FileCount ($entry.FileCount)\n",
    "  \"$($entry.BlockSize),$($entry.Buffercount),$($entry.MaxTransferSize),$($entry.FileCount),$($outcome.Duration.TotalSeconds)\" >> C:\\Temp\\PerfTest.txt\n",
    "    Remove-Item C:\\Temp\\BackupFiles\\*\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "As usual, we will first load the `tidyverse` package.  Then, we will load a new package, `randomForest`.  This lets us use a random forest model to analyze our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(!require(tidyverse)) {\n",
    "  install.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n",
    "  library(tidyverse)\n",
    "}\n",
    "\n",
    "if(!require(randomForest)) {\n",
    "  install.packages(\"randomForest\", repos = \"http://cran.us.r-project.org\")\n",
    "  library(randomForest)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using data from two databases, one which is approximately 1.5 GB in size and one which is approximately 17 GB in size.  Each file has the same set of variables in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backupstats_1g <- readr::read_csv(\"../Data/1.5GB PerfTest.txt\")\n",
    "backupstats_17g <- readr::read_csv(\"../Data/17 GB PerfTest.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to be able to combine the two sets of data together and draw conclusions across the broader scope.  Ideally, I'd have several databases of different sizes, but this is what I have to work with.  In order to differentiate the two sets of data, I have added in a new variable, DatabaseSize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backupstats_1g$DatabaseSize <- 1.5\n",
    "backupstats_17g$DatabaseSize <- 17\n",
    "backupstats <- rbind(backupstats_1g, backupstats_17g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help interpret the results a bit easier, I'm converting block size to kilobytes and max transfer size to 64 kilobyte chunks.  Otherwise, in the linear regression below, we'd have to think about the change in a single byte, and that's a bit crazy.  These are linear transformations of independent variables, so this change does not affect the end results aside from scaling the betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backupstats$BlockSizeKB <- backupstats$BlockSize / 1024.0\n",
    "backupstats$MaxTransferSize64KB <- backupstats$MaxTransferSize / (1024.0 * 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Random Forest\n",
    "\n",
    "I'd first like to try creating a random forest with this input data.  I'm going to create 2000 trees and will include importance information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model <- randomForest::randomForest(Duration ~ BlockSizeKB + BufferCount + MaxTransferSize64KB + FileCount + DatabaseSize,\n",
    "               data = backupstats,\n",
    "               ntree=2000,\n",
    "               importance=TRUE\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I included importance information, I can call the `importance` function to see which variables are most effective in describing duration.  By default, this function call returns two variables:  percent increase in mean squared error (`%IncMSE`) and increase in node purity `IncNodePurity`.  The increase in node purity is a biased measure which we should only use if `%IncMSE` is too expensive to calculate ([source](https://stats.stackexchange.com/questions/162465/in-a-random-forest-is-larger-incmse-better-or-worse)), so we will focus on the MSE changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest::importance(model, scale=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's really interesting about this result is that two variables are negative:  `BlockSizeKB` and `MaxTransferSize64KB`.  This means that, on my laptop and with this sample of two databases, those two parameters were useless.  We had an inkling that block size was not worthwhile, but I'm a bit surprised that max transfer size didn't make the cut, so to speak.\n",
    "\n",
    "The biggest contender was obviously database size:  larger databases take more time.  Next up is file count, telling me that a change in file count (we'll see which direction soon!) has the biggest impact.\n",
    "\n",
    "The last thing I want to look at is the percent of variance explained by the model, which I can see by just calling `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is pretty high:  we explained 87.08% of the **test** data set's variance.  If I were building a predictive model, I would want a separate test data set and compare my model's predictions against reality.  But for this case, I really am more interested in seeing what drives change rather than predicting the ideal parameters, so I'll keep moving.\n",
    "\n",
    "## A Tighter Model\n",
    "\n",
    "For the next model, I want to eliminate the two variables that we found to be useless:  block size and max transfer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 <- randomForest::randomForest(Duration ~ BufferCount + FileCount + DatabaseSize,\n",
    "               data = backupstats,\n",
    "               ntree=2000,\n",
    "               importance=TRUE\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this model, we can call `importance` again.  I'm going to pass in `type = 1` to show only `%IncMSE` this time around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest::importance(model2, scale=TRUE, type = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three variables are still positive, indicating that they're relevant variables in the model.  Let's see how well the model does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're explaining nearly 95% of the total training variance.  That's a pretty good score, though I'd be suspicious of overfitting--that's where the value of a separate test set really comes into play.  But let's leave that suspicion to the side for now and run some predictions.  I'd like to see what playing with each variable does, starting with buffer count.\n",
    "\n",
    "## Testing The Boundaries\n",
    "\n",
    "What I'm going to do next is keep two of my three variables fixed and modify the third to get a better understanding of what the overall model thinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_test <- data.frame(c(7, 15, 30, 60, 128, 256, 512, 1024), 4, 17)\n",
    "names(buffer_test) <- c(\"BufferCount\", \"FileCount\", \"DatabaseSize\")\n",
    "buffer_test$prediction <- predict(model2, buffer_test)\n",
    "buffer_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With buffer count changes, we go from approximately 79 seconds with the default buffer count down to about 70 seconds if we use 1024 buffers.  That's not a huge change, but it's certainly an improvement, and if this holds linearly, a 1.7 TB database would take roughly 15 minutes less time to back up (7000 seconds versus 7900 seconds).  Granted, the \"if this holds linearly\" is a heroic assumption...\n",
    "\n",
    "Anyhow, let's move on to look at file counts.  I'm going to try 1-6 files even though we only tested 1, 2, and 4 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_test <- data.frame(1024, 1:6, 17)\n",
    "names(file_test) <- c(\"BufferCount\", \"FileCount\", \"DatabaseSize\")\n",
    "file_test$prediction <- predict(model2, file_test)\n",
    "file_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we only have unique values at 1, 2, and 4 files.  The random forest algorithm does **not** extrapolate beyond the test range, meaning that anything above 4 files will be treated as 4 files.  Given the inputs, it's likely that there's also a test for [2, 4), so 3 gets caught up in that.\n",
    "\n",
    "Finally, let's look at database sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest does *not* do a regression here.\n",
    "size_test <- data.frame(1024, 4, 1:17)\n",
    "names(size_test) <- c(\"BufferCount\", \"FileCount\", \"DatabaseSize\")\n",
    "size_test$prediction <- predict(model2, size_test)\n",
    "size_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there is no regression (linear or otherwise) between these sizes.  The 1.5 GB size takes everything up to the halfway point, and 17 GB takes over from there.  If we want a linear model, let's build a linear model.\n",
    "\n",
    "## Contrast With A Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model <- lm(data = backupstats, Duration ~ BufferCount + FileCount + DatabaseSize)\n",
    "summary(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This linear model has an adjusted R^2 of 0.987, but it's basically telling us that time is a linear function of database size and that database size overwhelms everything else.  Let's look at some predictions for the size test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(linear_model, size_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single case with 1024 buffers and 4 files took 82 seconds, so that's not a bad prediction on that end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(backupstats_17g, BufferCount == 1024 & FileCount == 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the low end, a prediction of about 8.5 or so also looks pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(backupstats_1g, BufferCount == 1024 & FileCount == 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the full model, what's interesting is that for the linear model, max transfer size is slightly more valuable than buffer count in terms of p value (insert standard complaint about p values not being valuable here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm(data = backupstats, Duration ~ BlockSizeKB + BufferCount + MaxTransferSize64KB + FileCount + DatabaseSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the models by size, we can see that the database size was doing most of the heavy lifting.  Our model explains less than half of the 17 GB variance and under 20% of the 1.5 GB variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm(data = filter(backupstats, DatabaseSize == 17), \n",
    "           Duration ~ BlockSizeKB + BufferCount + MaxTransferSize64KB + FileCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm(data = filter(backupstats, DatabaseSize == 1.5), \n",
    "           Duration ~ BlockSizeKB + BufferCount + MaxTransferSize64KB + FileCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, the random forest explains 79% of the 17 GB variance and 27% of the 1.5 GB variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest::randomForest(Duration ~ BufferCount + FileCount,\n",
    "               data = filter(backupstats, DatabaseSize == 17),\n",
    "               ntree=2000,\n",
    "               importance=TRUE\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest::randomForest(Duration ~ BufferCount + FileCount,\n",
    "               data = filter(backupstats, DatabaseSize == 1.5),\n",
    "               ntree=2000,\n",
    "               importance=TRUE\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The moral here is that, within the confines of our two samples, database size ruled the roost.  Once you eliminate database size as a factor (by separating the \"large\" databases and the \"small\" databases from one another), the other factors aren't quite as powerful at explaining.  The good news with this is that optimizing this doesn't look like a \"knife-edge\" optimization problem, where being slightly off in your parameters means performance will be awful.  Rather, there appears to be a definite solution with a gradual shift.  It looks like file count is a great option for me (and that I should test with larger numbers of files), and one of buffer count or max transfer size is meaningful, but it seems not both at once.  I'm interpreting this as saying that I don't need 4 GB of RAM to take a backup of my 17 GB database, and therefore there are a number of buffer count * max transfer size values which get to about the correct amount of RAM for me.  Naturally, your results may differ, so you should try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
